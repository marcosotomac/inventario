# Sistema de Ingesta de Datos

Sistema de extracciÃ³n de datos de los microservicios y carga a AWS S3 (Data Lake).

## ğŸ“¦ Componentes

### 1. Ingesta de Productos

Extrae datos del microservicio de productos y los sube a S3.

**Datos extraÃ­dos:**

- Productos (CSV/JSON)
- CategorÃ­as (CSV)

**UbicaciÃ³n S3:**

- `s3://inventario-datalake/productos/productos_YYYYMMDD_HHMMSS.csv`
- `s3://inventario-datalake/categorias/categorias_YYYYMMDD_HHMMSS.csv`

### 2. Ingesta de Ã“rdenes

Extrae datos del microservicio de Ã³rdenes y clientes.

**Datos extraÃ­dos:**

- Ã“rdenes (CSV/JSON)
- Clientes (CSV/JSON)

**UbicaciÃ³n S3:**

- `s3://inventario-datalake/ordenes/ordenes_YYYYMMDD_HHMMSS.csv`
- `s3://inventario-datalake/clientes/clientes_YYYYMMDD_HHMMSS.csv`

### 3. Ingesta de Proveedores

Extrae datos del microservicio de proveedores (MongoDB).

**Datos extraÃ­dos:**

- Proveedores (CSV/JSON, aplanado de estructura JSON)

**UbicaciÃ³n S3:**

- `s3://inventario-datalake/proveedores/proveedores_YYYYMMDD_HHMMSS.csv`

## ğŸ”§ ConfiguraciÃ³n

### Variables de Entorno

```bash
# AWS
AWS_REGION=us-east-1
AWS_PROFILE=default
S3_BUCKET=inventario-datalake

# URLs de Servicios
PRODUCTOS_SERVICE_URL=http://productos-service:5001
ORDENES_SERVICE_URL=http://ordenes-service:8080
PROVEEDORES_SERVICE_URL=http://proveedores-service:3000
```

### Prerequisitos AWS

1. **Bucket S3 creado:**

```bash
aws s3 mb s3://inventario-datalake --region us-east-1
```

2. **AWS Credentials configuradas:**

```bash
aws configure --profile default
```

3. **Permisos IAM necesarios:**

- `s3:PutObject`
- `s3:GetObject`
- `s3:ListBucket`

## ğŸš€ Uso

### EjecuciÃ³n Individual

```bash
# Ingesta de productos
cd ingesta-productos
docker build -t ingesta-productos .
docker run --rm \
  -v ~/.aws:/root/.aws:ro \
  -e AWS_PROFILE=default \
  -e S3_BUCKET=inventario-datalake \
  --network inventario-network \
  ingesta-productos

# Ingesta de Ã³rdenes
cd ingesta-ordenes
docker build -t ingesta-ordenes .
docker run --rm \
  -v ~/.aws:/root/.aws:ro \
  -e AWS_PROFILE=default \
  -e S3_BUCKET=inventario-datalake \
  --network inventario-network \
  ingesta-ordenes

# Ingesta de proveedores
cd ingesta-proveedores
docker build -t ingesta-proveedores .
docker run --rm \
  -v ~/.aws:/root/.aws:ro \
  -e AWS_PROFILE=default \
  -e S3_BUCKET=inventario-datalake \
  --network inventario-network \
  ingesta-proveedores
```

### EjecuciÃ³n con Docker Compose

Ver el archivo `docker-compose.yml` principal en la raÃ­z del proyecto.

## ğŸ“Š Flujo de Datos

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Microservicios    â”‚
â”‚  - Productos        â”‚
â”‚  - Ã“rdenes          â”‚
â”‚  - Proveedores      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ Pull Strategy
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Contenedores de    â”‚
â”‚  Ingesta (Docker)   â”‚
â”‚  - Extract          â”‚
â”‚  - Transform        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ Upload
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    AWS S3 Bucket    â”‚
â”‚   (Data Lake)       â”‚
â”‚  - productos/       â”‚
â”‚  - ordenes/         â”‚
â”‚  - proveedores/     â”‚
â”‚  - clientes/        â”‚
â”‚  - categorias/      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   AWS Glue Crawler  â”‚
â”‚   (Catalogar)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚
           â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   AWS Athena        â”‚
â”‚   (Consultas SQL)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ”„ ProgramaciÃ³n AutomÃ¡tica

### OpciÃ³n 1: Cron en EC2

```bash
# Editar crontab
crontab -e

# Ejecutar ingesta diaria a las 2 AM
0 2 * * * /path/to/run-ingesta.sh >> /var/log/ingesta.log 2>&1
```

### OpciÃ³n 2: AWS EventBridge + Lambda

Crear funciones Lambda que ejecuten los contenedores de ingesta mediante ECS Tasks.

### OpciÃ³n 3: Airflow DAG

```python
from airflow import DAG
from airflow.operators.docker_operator import DockerOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'inventario',
    'depends_on_past': False,
    'start_date': datetime(2025, 10, 1),
    'email_on_failure': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'ingesta_inventario',
    default_args=default_args,
    description='Ingesta diaria de datos',
    schedule_interval='0 2 * * *',  # 2 AM diario
)

ingesta_productos = DockerOperator(
    task_id='ingesta_productos',
    image='ingesta-productos:latest',
    dag=dag,
)

ingesta_ordenes = DockerOperator(
    task_id='ingesta_ordenes',
    image='ingesta-ordenes:latest',
    dag=dag,
)

ingesta_proveedores = DockerOperator(
    task_id='ingesta_proveedores',
    image='ingesta-proveedores:latest',
    dag=dag,
)
```

## ğŸ“ Estructura del Proyecto

```
ingesta/
â”œâ”€â”€ ingesta-productos/
â”‚   â”œâ”€â”€ ingest.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ ingesta-ordenes/
â”‚   â”œâ”€â”€ ingest.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ ingesta-proveedores/
â”‚   â”œâ”€â”€ ingest.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ Dockerfile
â””â”€â”€ README.md
```

## ğŸ› Troubleshooting

**Error: No se puede conectar a los microservicios**

- Verificar que los servicios estÃ©n ejecutÃ¡ndose
- Verificar la red Docker
- Revisar las URLs de configuraciÃ³n

**Error: Credenciales AWS no encontradas**

```bash
# Verificar configuraciÃ³n
aws configure list

# Montar credenciales correctamente
docker run -v ~/.aws:/root/.aws:ro ...
```

**Error: Acceso denegado a S3**

- Verificar permisos IAM
- Verificar que el bucket exista
- Verificar la regiÃ³n configurada

## ğŸ’¡ Mejoras Futuras

- [ ] ValidaciÃ³n de esquemas con Great Expectations
- [ ] CompresiÃ³n de archivos (Gzip, Parquet)
- [ ] Particionamiento por fecha
- [ ] Logs centralizados
- [ ] MÃ©tricas de ingesta (CloudWatch)
- [ ] Manejo de datos incrementales
- [ ] DetecciÃ³n de duplicados
- [ ] Retry automÃ¡tico con backoff exponencial

## ğŸ“ Notas

- Los contenedores pueden funcionar sin S3 (guardan localmente en `/data`)
- Formato CSV para compatibilidad con Athena
- JSON adicional para backup y anÃ¡lisis
- ExtracciÃ³n paginada para manejar grandes volÃºmenes
- Timeouts configurados para evitar bloqueos
